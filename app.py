import streamlit as st
import pandas as pd
import google.generativeai as genai
from datetime import datetime

# --- CONFIGURATION ---
st.set_page_config(page_title="Assistant AGOrA", page_icon="üéì")
st.title("üéì Assistant PFMP AGOrA")

# R√©cup√©ration cl√©
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
except:
    st.error("Cl√© API manquante dans les Secrets.")
    st.stop()

# --- LE CERVEAU (PROMPT) ---
SYSTEM_PROMPT = """
Tu es un Assistant P√©dagogique Interactif (API), strictement d√©di√© √† l'entra√Ænement des √©l√®ves de Bac Pro AGOrA.
Ta mission : aider l‚Äô√©l√®ve √† structurer sa PFMP sans jamais faire le travail √† sa place.
R√àGLES : Ne r√©dige jamais √† sa place. Une seule question √† la fois. Ton encourageant.
"""

# --- S√âLECTION AUTOMATIQUE DU MOD√àLE (La partie magique) ---
# On ne force pas un nom, on cherche ce qui est disponible
if "valid_model_name" not in st.session_state:
    try:
        # On demande la liste des mod√®les disponibles pour CETTE cl√©
        available_models = list(genai.list_models())
        valid_models = []
        for m in available_models:
            # On garde ceux qui savent g√©n√©rer du texte
            if 'generateContent' in m.supported_generation_methods:
                valid_models.append(m.name)
        
        if not valid_models:
            st.error("‚ùå Aucun mod√®le accessible avec cette cl√©/r√©gion. V√©rifiez votre compte Google AI Studio.")
            st.stop()
        
        # On essaie de trouver un mod√®le "Flash" ou "Pro" en priorit√©
        chosen_model = None
        for m in valid_models:
            if "flash" in m and "1.5" in m:
                chosen_model = m
                break
        
        # Si pas de Flash, on prend le premier de la liste (ex: gemini-pro)
        if not chosen_model:
            chosen_model = valid_models[0]
            
        st.session_state["valid_model_name"] = chosen_model
        # On affiche un petit message discret pour savoir lequel a √©t√© choisi
        st.toast(f"Connect√© au mod√®le : {chosen_model}", icon="‚úÖ")

    except Exception as e:
        st.error(f"Erreur de connexion Google : {e}")
        st.stop()

# Configuration du mod√®le avec le nom trouv√© automatiquement
model = genai.GenerativeModel(
    model_name=st.session_state["valid_model_name"],
    system_instruction=SYSTEM_PROMPT
)

# --- GESTION DONN√âES & INTERFACE (Reste inchang√©) ---
if "conversation_log" not in st.session_state:
    st.session_state.conversation_log = []

def save_log(student_id, role, content):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    st.session_state.conversation_log.append({
        "Heure": timestamp,
        "Eleve": student_id,
        "Role": role,
        "Message": content
    })

with st.sidebar:
    st.header("Espace Professeur")
    student_id = st.text_input("Identifiant √âl√®ve :")
    if st.session_state.conversation_log:
        df = pd.DataFrame(st.session_state.conversation_log)
        csv = df.to_csv(index=False, sep=';').encode('utf-8-sig')
        st.download_button("üì• T√©l√©charger CSV", csv, "suivi_agora.csv", "text/csv")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Bonjour ! Je suis ton coach PFMP. Quelle activit√© veux-tu travailler ?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input("Ta r√©ponse..."):
    if not student_id:
        st.warning("‚ö†Ô∏è Entre ton identifiant √† gauche !")
    else:
        st.chat_message("user").write(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})
        save_log(student_id, "Eleve", prompt)

        try:
            # On nettoie l'historique pour √©viter les conflits de format
            history_gemini = []
            for m in st.session_state.messages:
                role = "user" if m["role"] == "user" else "model"
                history_gemini.append({"role": role, "parts": [m["content"]]})

            response = model.generate_content(history_gemini)
            bot_reply = response.text
            
            st.chat_message("assistant").write(bot_reply)
            st.session_state.messages.append({"role": "assistant", "content": bot_reply})
            save_log(student_id, "Assistant", bot_reply)
            
        except Exception as e:
            st.error(f"Erreur : {e}")
